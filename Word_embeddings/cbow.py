# -*- coding: utf-8 -*-
"""cbow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pmP8CPC8k7t8ztd9mbBE0WD19vcP6REQ
"""

# from google.colab import drive
# drive.mount('/content/drive')

import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize,word_tokenize
import numpy as np

from sklearn.decomposition import TruncatedSVD
import scipy
import re

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils import clip_grad_norm

device = "cuda" if torch.cuda.is_available() else "cpu"

with open("/content/drive/My Drive/input2.txt", "r") as f:
                       data=f.read()
                       data = re.sub(r'\.(?!\s)', '. ', data)
                       sent=sent_tokenize(data)

from sklearn.utils.validation import indexable
class CBOWDataset(Dataset):
    def __init__(self, sent, window_size):
        self.contexts,self.targets=self.load_sent(sent,window_size)
        self.window_size = window_size
        self.vocab_size=len(self.vocab)
       

    def load_sent(self,sent,window_size):
        freq={}
        t_sent=[]
        self.vocab=[]
        for sentence in sent:
            sentence = re.sub(r'\d+', 'NUM', sentence)
            words1 = word_tokenize(sentence)
            words=[word.lower() for word in words1]
            t_sent.append(words)
            for word in words:
                if word not in self.vocab:
                    self.vocab.append(word)
        self.word2index = {word: i for i, word in enumerate(self.vocab)}
        self.index2word = {i: word for word, i in enumerate(self.vocab)}
        contexts=[]
        targets=[]
        for words in t_sent:
                n=len(words)
                for i in range(window_size,n-window_size):
                    targets.append(self.word2index[words[i]])
                    con=words[i-window_size:i]+words[i+1:i+window_size+1]
                    contexts.append([self.word2index[w] for w in con])
        return contexts,targets            

    def __len__(self):
        return len(self.targets)

    def __getitem__(self, idx):
        context = self.contexts[idx]
        target = self.targets[idx]
        return torch.tensor(context), torch.tensor(target)

class CBOW(nn.Module):
    def __init__(self, vocab_size, embed_size):
        super(CBOW, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size,device=device)
        self.to(device)

    def forward(self, contexts,targets,batch_size,vocab_size,num_samples,window_size=2):
        # print(contexts.size())
        con_emb = self.embedding(contexts).mean(axis=1).unsqueeze(1)
        tar_emb =self.embedding(targets).unsqueeze(2)
        neg = torch.randint(low=0, high=vocab_size, size=(num_samples,targets.size()[0],2*window_size)).to(device)
        # print(con_emb.size())
        # print(tar_emb.size())
        neg_emb=self.embedding(neg).mean(axis=2)
        pos_product = torch.bmm(con_emb,tar_emb)
        # print(neg_emb.size())
        neg_loss=0
        poss_loss=0
        poss_loss=nn.functional.logsigmoid(pos_product).mean()
        for i in range(num_samples):
              #  print(neg_emb[i].unsqueeze(1).size())
               neg_product=torch.bmm(neg_emb[i].unsqueeze(1),tar_emb)
              #  print(neg_product.size())
               neg_loss+=nn.functional.logsigmoid(-neg_product).mean()
        
        return -(poss_loss+neg_loss)

dataset=CBOWDataset(sent,2)
dataloader= DataLoader(dataset, batch_size=50, shuffle=True, num_workers=2)

import math
    
def train(dataset,dataloader):
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = CBOW(dataset.vocab_size,50)
        model.train()
        optimiser = optim.Adam(model.parameters(), lr=0.01)
        # print(dataset.vocab_size)
        prev_loss=math.inf
        for epochs in range(8):
              epoch_loss=0
              j=0
              for context, target in dataloader:
                  j+=1
                  if j%50==0:

                    print(j)
                  context=context.to(device)
                  target=target.to(device)
                  optimiser.zero_grad()
                  loss = model(context,target,50,dataset.vocab_size,2).to(device)
                  # print("loss")
                  # print(loss)
                  epoch_loss+=loss
                  loss.backward()
                  clip_grad_norm_(model.parameters(), 1.0)
                  optimiser.step()
              print(epochs,epoch_loss/j)
              if epoch_loss-prev_loss>2.5:
                   return model
              prev_loss=epoch_loss
        return model

from torch.nn.utils import clip_grad_norm_

Train=0
if Train==1:
        model=train(dataset,dataloader)

        embedding_weights = model.embedding.weight.cpu().data.numpy()

        # count=0
        # with open("embed_cbow.text", 'w') as f:
        #        for w in embedding_weights:
        #          count+=1
        #          f.write(str(w.tolist())+'\n')
                
        # print(count)

        np.savez_compressed('cbow_matrix.npz', a=embedding_weights)



embedding_weights= np.load('cbow_matrix.npz')['a']

ind=dataset.word2index
def top_ten(word,embedding_weights,dataset):
  res = []
  w_ind=ind[word]
  w_vec=embedding_weights[w_ind]
  for i, embed in enumerate(embedding_weights):
              arr=[]
              d=scipy.spatial.distance.cosine(embed, w_vec)
              arr.append(d)
              arr.append(dataset.vocab[i])
              arr.append(embed)
              res.append(arr)
  sort_v1 = (sorted(res,key=lambda p:p[0]))[1:11]
  return sort_v1

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from sklearn.decomposition import TruncatedSVD
import scipy
import re
import matplotlib.cm as cm



plt.figure(figsize=(10, 10))
words=["wife","tried","you","have","slowly"]
colors = cm.rainbow(np.linspace(0, 1, len(words)))

for word, c in zip(words, colors):
    # print the top 10
    print("\nword:")
    print(word)
    top10 =top_ten(word,embedding_weights,dataset)
    print("top-words:")
    embeds=np.array([embedding_weights[ind[word]]])
    for x in top10:
        print(x[1])
        embeds = np.append(embeds, [x[2]], axis = 0)
    
  
    embeds = TSNE(n_components=2,perplexity=2).fit_transform(embeds)
    plt.scatter(embeds[:, 0], embeds[:, 1], color = c)
    plt.annotate(word, xy=(embeds[0, 0], embeds[0, 1]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')


plt.savefig('tsne3.png')

plt.figure(figsize=(10, 10))
words=["titanic"]
colors = cm.rainbow(np.linspace(0, 1, len(words)))

for word, c in zip(words, colors):
    print(word)
    top10 =top_ten(word,embedding_weights,dataset)
    embeds=np.array([embedding_weights[ind[word]]])
    for x in top10:
        print(x[1],x[0])
        embeds = np.append(embeds, [x[2]], axis = 0)
    
    
    embeds = TSNE(n_components=2,perplexity=2).fit_transform(embeds)
    plt.scatter(embeds[:, 0], embeds[:, 1], color = c)
    plt.annotate(word, xy=(embeds[0, 0], embeds[0, 1]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')

# save the plot
plt.savefig('tsne4.png')




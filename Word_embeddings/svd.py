# -*- coding: utf-8 -*-
"""svd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v-VOvwFtSvhfLxXofRLsJlzJ2ndymWFb
"""

# from google.colab import drive
# drive.mount('/content/drive')



import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize,word_tokenize
import numpy as np
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from sklearn.decomposition import TruncatedSVD
import scipy
import re
import matplotlib.cm as cm
train=0
if train==1:
      with open("/content/drive/My Drive/input2.txt", "r") as f:
                            data=f.read()
                            data = re.sub(r'\.(?!\s)', '. ', data)
                            sent=sent_tokenize(data)

      import string
      freq = {}
      vocab = []
      t_sent=[]
      for sentence in sent:
          sentence = re.sub(r'\d+', 'NUM', sentence)
          words1 = word_tokenize(sentence)
          words=[word.lower() for word in words1]
          t_sent.append(words)
          for word in words:
              if word not in vocab:
                  vocab.append(word)
                  freq[word] = 1
              else:
                  freq[word] += 1

      for word in vocab:
          if freq[word]<=3:
            # print(word)
            vocab.remove(word)
            del freq[word]

      l=len(vocab)
      ind={word:index for index,word in enumerate(vocab)}
      window=2
      matrix=np.zeros((l,l))
      matrix=matrix
      c=0
      for words in t_sent:
        c+=1
        if c%5000==0:
          print(c)
        n=len(words)
        for i in range(n):
            w1=words[i]
            if w1 in vocab:
              for j in range(1,window+1):
                  if i+j>=n:
                    break
                  w2=words[i+j]
                  if w2 in vocab:
                      matrix[ind[w1]][ind[w2]]+=1
                      matrix[ind[w2]][ind[w1]]+=1

      print(len(matrix))
      svd = TruncatedSVD(n_components=300)
      U = svd.fit_transform(matrix)	
      print(matrix.shape)
      print(U.shape)



      np.savez_compressed('svd_matrix.npz', a=U)



U = np.load('svd_matrix.npz')['a']

def top_ten(word,U):
  res = []
  w_ind=ind[word]
  w_vec=U[w_ind]
  for i, embed in enumerate(U):
              arr=[]
              d=scipy.spatial.distance.cosine(embed, w_vec)
              arr.append(d)
              arr.append(vocab[i])
              arr.append(embed)
              res.append(arr)
  sort_v1 = (sorted(res,key=lambda p:p[0]))[1:11]
  return sort_v1



print(vocab)
plt.figure(figsize=(10, 10))
words=["wife","tried","you","have","slowly"]
colors = cm.rainbow(np.linspace(0, 1, len(words)))

for word, c in zip(words, colors):
    # print the top 10
    print("\nword:")
    print(word)
    top10 =top_ten(word,U)
    print("top-words:")
    embeds=np.array([U[ind[word]]])
    for x in top10:
        print(x[1])
        embeds = np.append(embeds, [x[2]], axis = 0)
    
        
embeds = TSNE(n_components=2,perplexity=2).fit_transform(embeds)
plt.scatter(embeds[:, 0], embeds[:, 1], color = c)
plt.annotate(word, xy=(embeds[0, 0], embeds[0, 1]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')


plt.savefig('tsne1.png')

plt.figure(figsize=(10, 10))
words=["titanic"]
colors = cm.rainbow(np.linspace(0, 1, len(words)))

for word, c in zip(words, colors):
    print(word)
    top10 =top_ten(word,U)
    embeds=np.array([U[ind[word]]])
    for x in top10:
        print(x[1],x[0])
        embeds = np.append(embeds, [x[2]], axis = 0)


embeds = TSNE(n_components=2,perplexity=2).fit_transform(embeds)
plt.scatter(embeds[:, 0], embeds[:, 1], color = c)
plt.annotate(word, xy=(embeds[0, 0], embeds[0, 1]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')

# save the plot
plt.savefig('tsne2.png')

import gensim.downloader as api

      # Download the pre-trained Word2Vec model
model = api.load('word2vec-google-news-300')

x=model.most_similar('titanic', topn=10)
for i,j in x:
  print(i,j)


# -*- coding: utf-8 -*-
"""LSTM_POS_Tagging_with_PyTorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QRwwE8uN8rUQsGXfXHql2sP1eB52Qz3Z
"""

# import resources
import torch
import matplotlib.pyplot as plt
import gensim.downloader as api
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
import math
import torch.optim as optim



w2v_model = api.load("glove-wiki-gigaword-100")

# training sentences and their corresponding word-tags
import numpy as np
# Open the text file
def datap(name):
    with open(name, 'r') as f:
        # Initialize empty lists to store the words and tags
        words = []
        tags = []

        temp = []
        # Loop through each line in the file
        for line in f:

            # Split the line into words and tags
            tokens = line.strip().split()

            if(tokens[0] == "####"):

                data = (words, tags)
                words = []
                tags = []
                temp.append(data)
                continue

            # Append the words to the 'words' list
            words.append(tokens[1])
            # Append the tags to the 'tags' list
            tags.append(tokens[2])
        # Create a list of tuples containing the words and tags

    l_words = [word.lower() for word in words]

    data = ([l_words, tags])
    words = []
    tags = []
    temp.append(data)


    

    return temp;


# create a dictionary that maps words to indices
word2idx = {}

word_embeddings = []

training_data=datap("output_file.txt")
valid_data=datap("valid_file.txt")

print(training_data[0][0])
print(training_data[0][1])

for i in range(len(training_data)):
    sent=training_data[i][0]
    tags=training_data[i][1]
   
    
    for j in range(len(sent)):
        
        if sent[j] not in word2idx:

            try:
                # use pre-trained Word2Vec embedding if available
                embedding = w2v_model.wv[sent[j]]
            except KeyError:
                # assign a random embedding for OOV words
                embedding = np.random.normal(size=w2v_model.vector_size)
                
            word_embeddings.append(embedding)
            word2idx[sent[j]] = len(word2idx)
            


# add an entry for the OOV token
word2idx['<OOV>'] = len(word2idx)
word_embeddings.append(np.random.normal(size=w2v_model.vector_size))


word_embeddings = np.array(word_embeddings)

tag2idx = {'VERB':0, 'AUX':1, 'CCONJ':2, 'NUM':3, 'PART':4, 'NOUN':5, 'INTJ':6, 'DET':7, 'PRON':8, 'ADV':9, 'ADJ':10, 'ADP':11, 'PROPN':12}
idx2tag = {0:'VERB', 1:'AUX', 2:'CCONJ', 3:'NUM', 4:'PART', 5:'NOUN', 6:'INTJ', 7:'DET', 8:'PRON', 9:'ADV', 10:'ADJ', 11:'ADP', 12:'PROPN'}
tag2idx['<OOV>'] = len(tag2idx)
len1=len(word2idx)
len2=len(tag2idx)

def training(model,word2idx,training_data,tag2idx,loss_function,optimizer):

   epoch_loss=0.0

   for i in range(len(training_data)):
        
        sentence=training_data[i][0]
        tags=training_data[i][1]
        model.zero_grad()

        model.hidden = model.init_hidden()


        idxs=[]


        for w in sentence:
          if w in word2idx:
            idxs.append(word2idx[w])
          else:
            idxs.append(word2idx["<OOV>"])

        idxs = np.array(idxs)
        sentence_in=torch.from_numpy(idxs)

        idxs=[]

        for w in tags:
          if w in tag2idx:
            idxs.append(tag2idx[w])
          else:
            idxs.append(tag2idx["<OOV>"])

  
        idxs = np.array(idxs)
        targets=torch.from_numpy(idxs)
        
        tag_scores = model(sentence_in)

        loss = loss_function(tag_scores, targets)
        epoch_loss =epoch_loss+ loss.item()
        loss.backward()
        
        optimizer.step()

   res=epoch_loss/len(training_data)


   return res

  

def evaluating(model,word2idx,valid_data,tag2idx,loss_function,optimizer,idx2tag):
     epoch_loss=0.0

     
     for i in range(len(valid_data)):

        sentence=valid_data[i][0]
        tags=valid_data[i][1]
        
       
        model.zero_grad()

        
        model.hidden = model.init_hidden()

       
        idxs=[]


        for w in sentence:
          if w in word2idx:
            idxs.append(word2idx[w])
          else:
            idxs.append(word2idx["<OOV>"])

        idxs = np.array(idxs)
        sentence_in=torch.from_numpy(idxs)

        idxs=[]

        for w in tags:
          if w in tag2idx:
            idxs.append(tag2idx[w])
          else:
            idxs.append(tag2idx["<OOV>"])

  
        idxs = np.array(idxs)
        targets=torch.from_numpy(idxs)
       
        tag_scores = model(sentence_in)
        predicted_tags = [idx2tag[idx.item()] for idx in torch.argmax(tag_scores, dim=1)]

         
        loss = loss_function(tag_scores, targets)
        epoch_loss =epoch_loss+ loss.item()
        y_dec.extend(tags)
        y_dec1.extend(predicted_tags)


     res=epoch_loss/len(valid_data)

    
     
     return res,y_dec,y_dec1

class LSTMTagger(nn.Module):

    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):
      
        super(LSTMTagger, self).__init__()
        
        self.hidden_dim = hidden_dim

        # embedding layer that turns words into a vector of a specified size
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)

        # the LSTM takes embedded word vectors (of a specified size) as inputs 
        # and outputs hidden states of size hidden_dim
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)

        # the linear layer that maps the hidden state output dimension 
        # to the number of tags we want as output, tagset_size (in this case this is 3 tags)
        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)
        
        # initialize the hidden state (see code below)
        self.hidden = self.init_hidden()

        
    def init_hidden(self):
        ''' At the start of training, we need to initialize a hidden state;
           there will be none because the hidden state is formed based on perviously seen data.
           So, this function defines a hidden state with all zeroes and of a specified size.'''
        # The axes dimensions are (n_layers, batch_size, hidden_dim)
        return (torch.zeros(1, 1, self.hidden_dim),
                torch.zeros(1, 1, self.hidden_dim))

    def forward(self, sentence):
        ''' Define the feedforward behavior of the model.'''
        # create embedded word vectors for each word in a sentence
        embeds = self.word_embeddings(sentence)
        
        # get the output and hidden state by passing the lstm over our word embeddings
        # the lstm takes in our embeddings and hiddent state
        lstm_out, self.hidden = self.lstm(
            embeds.view(len(sentence), 1, -1), self.hidden)
        
        # get the scores for the most likely tag for a word
        tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))
        tag_scores = F.log_softmax(tag_outputs, dim=1)
        
        return tag_scores

model = LSTMTagger(6, 6, len1, len2)

loss_function = nn.NLLLoss()
temp3=0
optimizer = optim.SGD(model.parameters(), lr=0.1)
lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
n_epochs = 10
import math


best_valid_loss = float('inf')

for epoch in range(n_epochs):


     train_loss=training(model,word2idx,training_data,tag2idx,loss_function,optimizer)

     y_dec=[]
     y_dec1=[]
    
     valid_loss,y_dec,y_dec1=evaluating(model,word2idx,valid_data,tag2idx,loss_function,optimizer,idx2tag)

     

     acc = accuracy_score(y_dec, y_dec1)
     print("Accuracy:", acc)
     precision, recall, f1, support = precision_recall_fscore_support(y_dec, y_dec1, average='weighted')
     print("Precision:", precision)
     print("Recall:", recall)
     print("F1-score:", f1)
        



     lr_scheduler.step(valid_loss)

     if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'model_ulyssus.pt')

     print(f'\tTrain Perplexity: {math.exp(train_loss):.3f}')
     print(f'\tValid Perplexity: {math.exp(valid_loss):.3f}')

saved=True
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
torch.manual_seed(0)
test_data=datap("test_file.txt")
y_true = []
y_pred = []
if saved:
    model.load_state_dict(torch.load('model_ulyssus.pt',  map_location=device))

    test_loss,y_true,y_pred=evaluating(model,word2idx,test_data,tag2idx,loss_function,optimizer,idx2tag)

    print("Test Loss :",test_loss)


    acc = accuracy_score(y_true, y_pred)
    print("Accuracy:", acc)
    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average='weighted')
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1-score:", f1)
    

# Test Loss : 0.17995201347657216
# Accuracy: 0.9349667271627344
# Precision: 0.9360496488840522
# Recall: 0.9349667271627344
# F1-score: 0.9333986392349863

test_sentence = "i want a flight from nashville to seattle that arrives no later than 3 pm"
test_sentence=test_sentence.lower()
test_sentence=test_sentence.split()

# what	PRON
# 	is	AUX
# 	booking	NOUN
# 	class	NOUN
# 	c	SYM

test_data=datap('test_file.txt')
idxs=[]
for w in test_sentence:
  if w in word2idx:
    idxs.append(word2idx[w])
  else:
    idxs.append(word2idx["<OOV>"])
  
idxs = np.array(idxs)
inputs=torch.from_numpy(idxs)
tag_scores = model(inputs)



_, predicted_tags = torch.max(tag_scores, 1)
ff=[]
hh=predicted_tags.tolist()

for i in range(len(hh)):
  ff.append(idx2tag[hh[i]])

print(ff)

#['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'PROPN', 'ADP', 'PROPN', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NUM', 'NOUN']

